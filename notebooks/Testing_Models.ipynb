{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LLMs on Chinese National Social Work Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from typing import Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import qianfan\n",
    "import anthropic\n",
    "from mistralai import Mistral\n",
    "import google.generativeai as genai \n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API configuration for foundation models\n",
    "\n",
    "In this section we are configuring a total of eight foundation (web-based) models -- four Chinese models and four Western-centric models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API keys and configurations\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Common settings for all models\n",
    "COMMON_SETTINGS = {\n",
    "    \"temperature\": 0,  # Keep 0 for getting the most consistent answers\n",
    "    \"max_tokens\": 450,  # Comfortable space for answer + 150 characters explanation\n",
    "    \"frequency_penalty\": 0.1,  # Small penalty for better explanation readability\n",
    "    \"presence_penalty\": 0  # Keep 0 to stay focused on the question topic\n",
    "}\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"kimi\": {\n",
    "        \"api_key\": os.getenv('MOONSHOT_API_KEY'),\n",
    "        \"base_url\": \"https://api.moonshot.cn/v1\",\n",
    "        \"model_name\": \"moonshot-v1-8k\",\n",
    "        \"settings\": COMMON_SETTINGS\n",
    "    },\n",
    "\n",
    "    \"deepseek\": {\n",
    "        \"api_key\": os.getenv('DEEPSEEK_API_KEY'),\n",
    "        \"base_url\": \"https://api.deepseek.com\",\n",
    "        \"model_name\": \"deepseek-chat\",\n",
    "        \"settings\": COMMON_SETTINGS\n",
    "    },\n",
    "\n",
    "    \"qwen\": {\n",
    "        \"api_key\": os.getenv('QWEN_API_KEY'),\n",
    "        \"base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        \"model_name\": \"qwen-max\",\n",
    "        \"settings\": COMMON_SETTINGS\n",
    "    },\n",
    "\n",
    "    \"ernie\": {\n",
    "        \"access_key\": os.getenv('QIANFAN_ACCESS_KEY'),\n",
    "        \"secret_key\": os.getenv('QIANFAN_SECRET_KEY'),\n",
    "        \"model_name\": \"ERNIE-4.0-8K\",\n",
    "        \"settings\": COMMON_SETTINGS\n",
    "    },\n",
    "\n",
    "    \"gpt\": {\n",
    "        \"api_key\": os.getenv('OPENAI_API_KEY'),\n",
    "        \"base_url\": None,\n",
    "        \"model_name\": \"gpt-4o\",\n",
    "        \"settings\": COMMON_SETTINGS\n",
    "    },\n",
    "\n",
    "    \"anthropic\": {\n",
    "        \"api_key\": os.getenv('ANTHROPIC_API_KEY'),\n",
    "        \"base_url\": \"https://api.anthropic.com/v1\",\n",
    "        \"model_name\": \"claude-3-5-sonnet-20241022\",\n",
    "        \"settings\": COMMON_SETTINGS\n",
    "    },\n",
    "\n",
    "    \"mistral\": {\n",
    "        \"api_key\": os.getenv('MISTRAL_API_KEY'),\n",
    "        \"base_url\": \"https://api.mistral.ai/v1/chat/completions\",\n",
    "        \"model_name\": \"mistral-large-latest\",\n",
    "        \"settings\": COMMON_SETTINGS\n",
    "    },\n",
    "\n",
    "    \"gemini\": {\n",
    "        \"api_key\": os.getenv('GEMINI_API_KEY'),\n",
    "        \"model_name\": \"gemini-1.5-pro\",\n",
    "        \"settings\": COMMON_SETTINGS\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Configurations\n",
    "\n",
    "This code defines a structured configuration for an exam-taking assistant system in Chinese. The configurations are organized into two distinct scenarios: single-choice questions and multiple-choice questions. For each type, there's a system prompt that establishes the AI's role as an exam expert and specifies the exact response format, followed by a user prompt template that will be filled with the actual question and answer choices. The general configuration allows for flexible answer selection, the single-choice configuration enforces selection of exactly one answer while explaining why others were rejected, and the multiple-choice configuration requires selecting all correct answers with justification for both selected and unselected options. Each prompt template requires responses to follow a strict format with the selected answer(s) and a roughly 150-character explanation of the reasoning. The templates use string formatting placeholders for dynamic insertion of questions and answer choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts(prompts_dir=\"/user_prompts\"):\n",
    "    \"\"\"Load all txt files as prompts and use filenames (without .txt) as keys\"\"\"\n",
    "    EXAM_PROMPTS = {}\n",
    "    \n",
    "    # Get all txt files in the directory\n",
    "    txt_files = [f for f in os.listdir(prompts_dir) if f.endswith('.txt')]\n",
    "    \n",
    "    for filename in txt_files:\n",
    "        file_path = os.path.join(prompts_dir, filename)\n",
    "        prompt_key = filename[:-4]  # Remove .txt extension\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().strip()\n",
    "                \n",
    "                # Assume each file contains both system and user prompts separated by a delimiter\n",
    "                if '\\n---\\n' in content:  # Example delimiter\n",
    "                    system_prompt, user_prompt = content.split('\\n---\\n')\n",
    "                else:\n",
    "                    # Default system prompt if not specified in file\n",
    "                    system_prompt = \"你是一位精通中国大陆社会政策和社会工作领域的专家, 你正在参加中国社会工作者职业水平考试。你只能以指定的JSON格式回答，不能有任何其他对话或说明。\"\n",
    "                    user_prompt = content\n",
    "                \n",
    "                EXAM_PROMPTS[prompt_key] = {\n",
    "                    \"system\": system_prompt.strip(),\n",
    "                    \"user\": user_prompt.strip()\n",
    "                }\n",
    "                print(f\"Loaded prompt: {prompt_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {str(e)}\")\n",
    "    \n",
    "    return EXAM_PROMPTS\n",
    "\n",
    "# Load prompts\n",
    "EXAM_PROMPTS = load_prompts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_response(response: str, prompt_name: str = \"\") -> tuple:\n",
    "    \"\"\"Parse model response from JSON format with Chinese keys\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Clean the response string\n",
    "        response = response.strip()\n",
    "        if response.startswith('\\n'):\n",
    "            response = response.lstrip('\\n')\n",
    "        \n",
    "        # Look for JSON structure\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}') + 1\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            json_str = response[start_idx:end_idx]\n",
    "            \n",
    "            try:\n",
    "                parsed = json.loads(json_str)\n",
    "                \n",
    "                if prompt_name.startswith(\"condition_4\"):\n",
    "                    return (\n",
    "                        parsed.get('答案', ''), parsed.get('理由', '')\n",
    "                    )\n",
    "                return parsed.get('答案', ''), parsed.get('理由', ''), parsed.get('信心', '')\n",
    "            \n",
    "            except json.JSONDecodeError as je:\n",
    "                print(f\"\\nJSON decode error: {je}\")\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(\"No JSON structure found in response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in parse_model_response: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultCreator:\n",
    "    \"\"\"Helper class to manage result creation\"\"\"\n",
    "    def __init__(self, model_name, prompt_name, timestamp):\n",
    "        self.model_name = model_name\n",
    "        self.prompt_name = prompt_name\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "    def create_dict(self, row, model_response):\n",
    "        \"\"\"Create result dictionary with current settings\"\"\"\n",
    "        base_dict = {\n",
    "            'Question_ID': row['Question_ID'],\n",
    "            'Question': row['Question'],\n",
    "            'Selections': row['Selections'],\n",
    "            'Correct_Answer': row['Answer'],\n",
    "            'Official_Explanation': row['Explanation'],\n",
    "            'Model': self.model_name,\n",
    "            'Prompt': self.prompt_name,\n",
    "            'Timestamp': self.timestamp\n",
    "        }\n",
    "        \n",
    "        # Parse response and update dict\n",
    "        if self.prompt_name.startswith(\"condition_4\"):\n",
    "            answer, explanation = parse_model_response(model_response, self.prompt_name)\n",
    "            base_dict.update({\n",
    "                'Model_Answer': answer,\n",
    "                'Model_Explanation': explanation\n",
    "            })\n",
    "        else:\n",
    "            answer, explanation, confidence = parse_model_response(model_response, self.prompt_name)\n",
    "            base_dict.update({\n",
    "                'Model_Answer': answer,\n",
    "                'Model_Explanation': explanation,\n",
    "                'Model_Confidence': confidence\n",
    "            })\n",
    "            \n",
    "        \n",
    "        return base_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_exam(exam_df: pd.DataFrame, model_name: str, prompt_name: str, df_name: str = \"sample\") -> pd.DataFrame:\n",
    "    \"\"\"Process exam using specified model and prompt\"\"\"\n",
    "    if model_name not in MODEL_CONFIGS:\n",
    "        raise ValueError(f\"Model '{model_name}' not found. Available models: {list(MODEL_CONFIGS.keys())}\")\n",
    "    if prompt_name not in EXAM_PROMPTS:\n",
    "        raise ValueError(f\"Prompt '{prompt_name}' not found. Available prompts: {list(EXAM_PROMPTS.keys())}\")\n",
    "        \n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "    template = EXAM_PROMPTS[prompt_name]\n",
    "    \n",
    "    # Initialize client based on model type\n",
    "    if model_name == \"gpt\":\n",
    "        client = OpenAI(api_key=config[\"api_key\"])\n",
    "    else:\n",
    "        client = OpenAI(\n",
    "            api_key=config[\"api_key\"],\n",
    "            base_url=config[\"base_url\"]\n",
    "        )\n",
    "    \n",
    "    results = []\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    total_questions = len(exam_df)\n",
    "    \n",
    "    print(f\"Starting to process {total_questions} questions...\")\n",
    "\n",
    "    # Initialize ResultCreator\n",
    "    result_creator = ResultCreator(model_name, prompt_name, timestamp)\n",
    "    \n",
    "    for index, row in exam_df.iterrows():\n",
    "        try:\n",
    "            user_prompt = template[\"user\"].format(\n",
    "                question=row['Question'],\n",
    "                selections=row['Selections']\n",
    "            )\n",
    "            \n",
    "            completion = client.chat.completions.create(\n",
    "                model=config[\"model_name\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": template[\"system\"]},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                **config[\"settings\"]\n",
    "            )\n",
    "            \n",
    "            model_response = completion.choices[0].message.content\n",
    "            \n",
    "            results.append(result_creator.create_dict(row, model_response))\n",
    "           \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {row['Question_ID']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    os.makedirs(\"frontier_results_datafiles\", exist_ok=True) \n",
    "    filename = os.path.join(\"frontier_results_datafiles\", f\"results_{df_name}_{model_name}_{prompt_name}.csv\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nResults saved to: {filename}\")\n",
    "    print(f\"Total questions processed: {len(results)}/{total_questions}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_exam_other(exam_df: pd.DataFrame, model_name: str, prompt_name: str, df_name: str = \"sample\") -> pd.DataFrame:\n",
    "    \"\"\"Process exam using ERNIE, Claude, or Mistral\"\"\"\n",
    "    print(f\"\\nStarting process with model: {model_name}, prompt: {prompt_name}\")\n",
    "    \n",
    "    # Setup\n",
    "    template = EXAM_PROMPTS[prompt_name]\n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "    result_creator = ResultCreator(model_name, prompt_name, datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    results = []\n",
    "    \n",
    "    # Initialize client\n",
    "    if model_name == \"ernie\":\n",
    "        os.environ[\"QIANFAN_ACCESS_KEY\"] = config[\"access_key\"]\n",
    "        os.environ[\"QIANFAN_SECRET_KEY\"] = config[\"secret_key\"]\n",
    "        client = qianfan.ChatCompletion()\n",
    "    elif model_name == \"anthropic\":\n",
    "        client = anthropic.Anthropic(api_key=config[\"api_key\"])\n",
    "    elif model_name == \"mistral\":\n",
    "        client = Mistral(api_key=config[\"api_key\"])\n",
    "    elif model_name == \"gemini\":\n",
    "        genai.configure(api_key=config[\"api_key\"])\n",
    "        client = genai.GenerativeModel('gemini-1.5-pro',\n",
    "                                       system_instruction=template[\"system\"])\n",
    "    \n",
    "    print(f\"Starting to process {len(exam_df)} questions...\")\n",
    "    \n",
    "    for index, row in exam_df.iterrows():\n",
    "        try:\n",
    "            user_prompt = template[\"user\"].format(\n",
    "                question=row['Question'],\n",
    "                selections=row['Selections']\n",
    "            )\n",
    "            \n",
    "            # Get model response based on model type\n",
    "            if model_name == \"ernie\":\n",
    "                response = client.do(\n",
    "                    model=config[\"model_name\"],\n",
    "                    system=template[\"system\"],\n",
    "                    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    temperature=0.1,\n",
    "                )\n",
    "                model_response = response[\"body\"][\"result\"]\n",
    "                \n",
    "            elif model_name == \"anthropic\":\n",
    "                response = client.messages.create(\n",
    "                    model=config[\"model_name\"],\n",
    "                    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                    system=template[\"system\"],\n",
    "                    temperature=config[\"settings\"][\"temperature\"],\n",
    "                    max_tokens=config[\"settings\"][\"max_tokens\"]\n",
    "                )\n",
    "                model_response = response.content[0].text\n",
    "                \n",
    "            elif model_name == \"mistral\":\n",
    "                response = client.chat.complete(\n",
    "                    model=config[\"model_name\"],\n",
    "                    messages=[{\"role\": \"user\", \"content\": f\"{template['system']}\\n\\n{user_prompt}\"}],\n",
    "                    temperature=config[\"settings\"][\"temperature\"],\n",
    "                    max_tokens=config[\"settings\"][\"max_tokens\"]\n",
    "                )\n",
    "                model_response = response.choices[0].message.content\n",
    "            \n",
    "            elif model_name == \"gemini\":\n",
    "                response = client.generate_content(\n",
    "                    user_prompt,\n",
    "                    generation_config={\n",
    "                        \"temperature\": config[\"settings\"][\"temperature\"],\n",
    "                        \"max_output_tokens\": config[\"settings\"][\"max_tokens\"]\n",
    "                    }\n",
    "                )\n",
    "                model_response = response.text\n",
    "            \n",
    "            results.append(result_creator.create_dict(row, model_response))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {row['Question_ID']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    os.makedirs(\"frontier_results_datafiles\", exist_ok=True)\n",
    "    filename = os.path.join(\"frontier_results_datafiles\", f\"results_{df_name}_{model_name}_{prompt_name}.csv\")\n",
    "    results_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nProcessed {len(results)}/{len(exam_df)} questions\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "Models are tested by the following conditions on both Jurisprudence and Applied Knowledge sections:<br/>\n",
    "- Condition 1: Models received explicit instructions regarding question type (single-select or SATA) and were directed to answer every question<br/>\n",
    "- Condition 2: While maintaining similar procedures to Condition 1, models could skip questions they couldn't answer with reasonable confidence<br/>\n",
    "- Condition 3: Models are presented with only answer options, withholding the actual question stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare exam data\n",
    "policy_exam_df = pd.read_csv(f'exam_datafiles/2023_policy_exam.csv')\n",
    "comprehensive_exam_df = pd.read_csv(f'exam_datafiles/2023_comprehensive_exam.csv')\n",
    "\n",
    "policy_exam_single = policy_exam_df.iloc[:60]\n",
    "policy_exam_multiple = policy_exam_df.iloc[60:]\n",
    "\n",
    "comprehensive_exam_single = comprehensive_exam_df.iloc[:60]\n",
    "comprehensive_exam_multiple = comprehensive_exam_df.iloc[60:]\n",
    "\n",
    "print(\"Policy Exam Questions:\", len(policy_exam_df))\n",
    "print(\"Comprehensive Exam Questions:\", len(comprehensive_exam_df))\n",
    "print(\"Available prompts:\", list(EXAM_PROMPTS.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Condition 1 - with Jurisprudence(Policy) Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [policy_exam_multiple, policy_exam_single]\n",
    "model_names = [\"deepseek\", \"gpt\", \"qwen\", \"kimi\"]\n",
    "df_name = \"policy\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_1_single\" if exam_df is policy_exam_single else \"condition_1_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [policy_exam_multiple,policy_exam_single]\n",
    "model_names = [\"mistral\", \"gemini\"\"ernie\", \"anthropic\"]\n",
    "df_name = \"policy\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_1_single\" if exam_df is policy_exam_single else \"condition_1_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam_other(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Condition 1 - with Applied Skills(Comprehensive) Quesitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [comprehensive_exam_multiple, comprehensive_exam_single]\n",
    "model_names = [\"qwen\", \"deepseek\", \"gpt\", \"kimi\"]\n",
    "df_name = \"comprehensive\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_1_single\" if exam_df is comprehensive_exam_single else \"condition_1_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [comprehensive_exam_multiple, comprehensive_exam_single]\n",
    "model_names = [\"mistral\", \"ernie\", \"anthropic\", \"gemini\"]\n",
    "df_name = \"comprehensive\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_1_single\" if exam_df is comprehensive_exam_single else \"condition_1_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam_other(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Condition 2 - with Jurisprudence(Policy) Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [policy_exam_multiple, policy_exam_single]\n",
    "model_names = [\"qwen\", \"kimi\",\"deepseek\", \"gpt\"] \n",
    "df_name = \"policy\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_2_single\" if exam_df is policy_exam_single else \"condition_2_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [policy_exam_multiple, policy_exam_single]\n",
    "model_names = [\"ernie\", \"mistral\", \"anthropic\", \"gemini\"]\n",
    "df_name = \"policy\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_2_single\" if exam_df is policy_exam_single else \"condition_2_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam_other(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Condition 2 - with Applied Skills(Comprehensive) Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [comprehensive_exam_multiple, comprehensive_exam_single]\n",
    "model_names = [\"qwen\", \"kimi\",\"deepseek\", \"gpt\"] \n",
    "df_name = \"comprehensive\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_2_single\" if exam_df is comprehensive_exam_single else \"condition_2_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [comprehensive_exam_multiple, comprehensive_exam_single]\n",
    "model_names = [\"ernie\", \"mistral\", \"anthropic\", \"gemini\"]\n",
    "df_name = \"comprehensive\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_2_single\" if exam_df is comprehensive_exam_single else \"condition_2_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam_other(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Condition 3 - with Jurisprudence(Policy) Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [policy_exam_multiple, policy_exam_single] \n",
    "model_names = [\"qwen\", \"kimi\",\"deepseek\", \"gpt\"] \n",
    "df_name = \"policy\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_3_single\" if exam_df is policy_exam_single else \"condition_3_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [policy_exam_multiple, policy_exam_single]\n",
    "model_names = [\"ernie\", \"mistral\", \"anthropic\", \"gemini\"]\n",
    "df_name = \"policy\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_3_single\" if exam_df is policy_exam_single else \"condition_3_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam_other(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Condition 3 - with Applied Skills(Comprehensive) Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [comprehensive_exam_multiple, comprehensive_exam_single]\n",
    "model_names = [\"qwen\", \"kimi\",\"deepseek\", \"gpt\"] \n",
    "df_name = \"comprehensive\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_3_single\" if exam_df is comprehensive_exam_single else \"condition_3_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables for the function parameters\n",
    "exam_dfs = [comprehensive_exam_multiple, comprehensive_exam_single]\n",
    "model_names = [\"ernie\", \"mistral\", \"anthropic\", \"gemini\"]\n",
    "df_name = \"comprehensive\"\n",
    "\n",
    "# Loop through all combinations and execute the function\n",
    "for exam_df in exam_dfs:\n",
    "    # Set prompt_name based on the DataFrame being used\n",
    "    prompt_name = \"condition_3_single\" if exam_df is comprehensive_exam_single else \"condition_3_multiple\"\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        process_exam_other(exam_df=exam_df, model_name=model_name, prompt_name=prompt_name, df_name=df_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base_research_env)",
   "language": "python",
   "name": "base_research_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
