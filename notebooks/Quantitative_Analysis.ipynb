{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Analysis on LLMs Performance\n",
    "This notebook performs comprehensive quantitative analysis on LLM performance across different test conditions. Functions are defined to load and process data, calculate scores, and generate statistics. The main execution block loads data, calculates scores, and saves the results to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_model_answer(answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean Model_Answer by removing spaces and commas\n",
    "    Example: 'A, B, C' -> 'A B C'\n",
    "    \"\"\"\n",
    "    if pd.isna(answer):  # Handle NaN values\n",
    "        return answer\n",
    "    return answer.replace(' ', '').replace(',', '')\n",
    "\n",
    "def clean_confidence_value(x):\n",
    "    \"\"\"\n",
    "    Convert confidence values to float, handling various formats\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            # Remove any non-numeric characters except decimal point\n",
    "            x = ''.join(c for c in x if c.isdigit() or c == '.')\n",
    "        return float(x)\n",
    "    except (ValueError, TypeError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_files() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load all CSV files and organize them by condition\n",
    "    Returns a dictionary with condition numbers as keys and corresponding dataframes as values\n",
    "    \"\"\"\n",
    "    # Dictionary to store dataframes by condition number\n",
    "    condition_dfs = {}\n",
    "    \n",
    "    # Get all CSV files\n",
    "    csv_files = glob.glob(r'frontier_results_datafiles/results_*.csv')\n",
    "    \n",
    "    for file in csv_files:\n",
    "        # Get just the filename without the path\n",
    "        filename = os.path.basename(file)\n",
    "        # Parse filename components\n",
    "        parts = filename.replace('.csv', '').split('_')\n",
    "        \n",
    "        if len(parts) != 6:\n",
    "            print(f\"Skipping {file} - not enough parts in filename\")\n",
    "            continue\n",
    "            \n",
    "        exam_type = parts[1]      # policy or comprehensive\n",
    "        model = parts[2]          # qwen, mistral, etc.\n",
    "        quesion_type = parts[5]   # \n",
    "\n",
    "        # Find the index of \"condition\" in parts and get the number after it\n",
    "        try:\n",
    "            condition_index = parts.index('condition')\n",
    "            condition_num = parts[condition_index + 1]\n",
    "            condition = f\"condition_{condition_num}\"\n",
    "        except ValueError:\n",
    "            print(f\"Could not find condition number in {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Skip reading files for condition_3\n",
    "        if condition == 'condition_3':\n",
    "            print(f\"Skipping {file} - condition_3\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing: {filename}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Clean Model_Answer column\n",
    "        df['Model_Answer'] = df['Model_Answer'].apply(clean_model_answer)\n",
    "        if condition != 'condition_3':\n",
    "            df['Model_Confidence'] = df['Model_Confidence'].apply(clean_confidence_value)\n",
    "        \n",
    "        # Add exam_type from filename\n",
    "        df['exam_type'] = exam_type\n",
    "        df[\"question_type\"] = quesion_type\n",
    "        \n",
    "        # Initialize condition key if it doesn't exist\n",
    "        if condition not in condition_dfs:\n",
    "            condition_dfs[condition] = []\n",
    "        \n",
    "        # Append the dataframe to the appropriate condition list\n",
    "        condition_dfs[condition].append(df)\n",
    "    \n",
    "    # Combine dataframes within each condition\n",
    "    for condition in list(condition_dfs.keys()):\n",
    "        if condition_dfs[condition]:\n",
    "            condition_dfs[condition] = pd.concat(condition_dfs[condition], ignore_index=True)\n",
    "            print(f\"Concatenated {condition}: {len(condition_dfs[condition])} rows\")\n",
    "        else:\n",
    "            print(f\"Warning: No files found for {condition}\")\n",
    "            condition_dfs.pop(condition)\n",
    "    \n",
    "    print(\"\\nAvailable conditions:\", list(condition_dfs.keys()))\n",
    "    return condition_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(condition_name: str, condition_dfs: Dict[str, pd.DataFrame], include_confidence: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Universal function to calculate scores for all conditions.\n",
    "    \"\"\"\n",
    "    if condition_name not in condition_dfs:\n",
    "        print(f\"Error: {condition_name} not found in the data\")\n",
    "        return None\n",
    "        \n",
    "    df = condition_dfs[condition_name].copy()  # Create a copy to avoid modifying original\n",
    "\n",
    "    # Add skipped answer flagging for condition 2\n",
    "    if condition_name == 'condition_2':\n",
    "        df['is_skipped'] = df['Model_Answer'].astype(str).str.contains('跳过')\n",
    "    \n",
    "    # Convert Question_ID to numeric, removing any non-numeric characters\n",
    "    df['Question_ID'] = pd.to_numeric(df['Question_ID'].astype(str).str.extract('(\\d+)', expand=False))\n",
    "    \n",
    "    def calculate_sata_score_and_correctness(row):\n",
    "        \"\"\"\n",
    "        Calculate score and correctness type for a SATA question\n",
    "        Scoring rules:\n",
    "        - Exact match: 2 points\n",
    "        - Partial match: 0.5 points per correct selection\n",
    "        - Any incorrect selection: 0 points\n",
    "        \"\"\"\n",
    "        correct_ans = set(str(row['Correct_Answer']))\n",
    "        model_ans = set(str(row['Model_Answer']))\n",
    "        \n",
    "        if any(ans not in correct_ans for ans in model_ans):\n",
    "            return 0, 'incorrect'\n",
    "        \n",
    "        correct_selections = len(model_ans.intersection(correct_ans))\n",
    "        \n",
    "        if correct_selections == len(correct_ans) and len(model_ans) == len(correct_ans):\n",
    "            return 2, 'exact'\n",
    "        elif correct_selections > 0:\n",
    "            return 0.5 * correct_selections, 'partial'\n",
    "        else:\n",
    "            return 0, 'incorrect'\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model in df['Model'].unique():\n",
    "        for exam in df['exam_type'].unique():\n",
    "            model_exam_mask = (df['Model'] == model) & (df['exam_type'] == exam)\n",
    "            \n",
    "            # Calculate skipped questions count for condition 2\n",
    "            skipped_count = 0\n",
    "            if condition_name == 'condition_2':\n",
    "                skipped_count = df[model_exam_mask]['is_skipped'].sum()\n",
    "            \n",
    "            # Single select calculations\n",
    "            single_mask = model_exam_mask & (df['question_type'] == 'single')\n",
    "            single_df = df[single_mask]\n",
    "            \n",
    "            single_total = len(single_df)\n",
    "            single_exact_correct = single_df['Model_Answer'].eq(single_df['Correct_Answer']).sum()\n",
    "            single_exact_percentage = (single_exact_correct / single_total * 100) if single_total > 0 else 0\n",
    "            \n",
    "            # SATA calculations\n",
    "            multiple_mask = model_exam_mask & (df['question_type'] == 'multiple')\n",
    "            multiple_df = df[multiple_mask]\n",
    "            \n",
    "            multiple_results = [calculate_sata_score_and_correctness(row) for _, row in multiple_df.iterrows()]\n",
    "            multiple_scores, multiple_correctness = zip(*multiple_results) if multiple_results else ([], [])\n",
    "            \n",
    "            multiple_total = len(multiple_df)\n",
    "            multiple_exact_correct = multiple_correctness.count('exact')\n",
    "            multiple_any_correct = multiple_correctness.count('exact') + multiple_correctness.count('partial')\n",
    "            \n",
    "            multiple_exact_percentage = (multiple_exact_correct / multiple_total * 100) if multiple_total > 0 else 0\n",
    "            multiple_any_correct_percentage = (multiple_any_correct / multiple_total * 100) if multiple_total > 0 else 0\n",
    "            \n",
    "            # Calculate total scores\n",
    "            total_single_score = single_exact_correct\n",
    "            total_multiple_score = sum(multiple_scores)\n",
    "            total_score = total_single_score + total_multiple_score\n",
    "            \n",
    "            # Prepare results dictionary\n",
    "            result_dict = {\n",
    "                'exam_type': exam,\n",
    "                'Model': model,\n",
    "                'total_score(out of 100)': total_score,\n",
    "                'correct_percentage': (single_exact_correct + multiple_exact_correct) / 80,\n",
    "                'single_correct_percentage': single_exact_percentage/100,\n",
    "                'multiple_exact_correct_percentage': multiple_exact_percentage/100,\n",
    "                'multiple_include_partial_correct_percentage': multiple_any_correct_percentage/100,\n",
    "                'single_score(out of 60)': total_single_score,\n",
    "                'multiple_score(out of 40)': total_multiple_score,\n",
    "                'single_questions_count': single_total,\n",
    "                'multiple_questions_count': multiple_total,\n",
    "                'single_correct_count': single_exact_correct,\n",
    "                'multiple_exact_correct_count': multiple_exact_correct,\n",
    "                'total_correct_count': single_exact_correct + multiple_exact_correct\n",
    "            }\n",
    "            \n",
    "            # Add skipped questions count for condition 2\n",
    "            if condition_name == 'condition_2':\n",
    "                result_dict['skipped_questions_count'] = skipped_count\n",
    "            \n",
    "            # Add confidence scores if available and requested\n",
    "            if include_confidence and 'Model_Confidence' in df.columns:\n",
    "                try:\n",
    "                    result_dict.update({\n",
    "                        'avg_confidence': df[model_exam_mask]['Model_Confidence'].mean()\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error calculating confidence for {model}, {exam}: {str(e)}\")\n",
    "            \n",
    "            results.append(result_dict)\n",
    "    \n",
    "    # Convert to DataFrame and round values\n",
    "    scores_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Round numeric columns\n",
    "    numeric_columns = scores_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    scores_df[numeric_columns] = scores_df[numeric_columns].round(3)\n",
    "    \n",
    "    # Sort by total score descending within each exam type\n",
    "    scores_df = scores_df.sort_values(['exam_type', 'total_score(out of 100)'], ascending=[True, False])\n",
    "    \n",
    "    return scores_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_detailed_statistics(condition_dfs):\n",
    "    stats_data = []\n",
    "    format_accuracy_data = []\n",
    "    \n",
    "    # 1. Stats by exam type and condition\n",
    "    for condition, df in condition_dfs.items():\n",
    "        df_scores = calculate_scores(condition, condition_dfs)\n",
    "        for exam_type in ['policy', 'comprehensive']:\n",
    "            exam_df = df_scores[df_scores['exam_type'] == exam_type]\n",
    "            stats_dict = {\n",
    "                'analysis_type': 'exam_type',\n",
    "                'condition': condition,\n",
    "                'exam_type': exam_type,\n",
    "                'region': 'all',\n",
    "                'avg_total': exam_df['total_score(out of 100)'].mean(),\n",
    "                'median_total': exam_df['total_score(out of 100)'].median(),\n",
    "                'std_total': exam_df['total_score(out of 100)'].std(),\n",
    "                'avg_single': exam_df['single_score(out of 60)'].mean(),\n",
    "                'median_single': exam_df['single_score(out of 60)'].median(),\n",
    "                'avg_multiple': exam_df['multiple_score(out of 40)'].mean(),\n",
    "                'median_multiple': exam_df['multiple_score(out of 40)'].median(),\n",
    "                'avg_single_percentage': exam_df['single_correct_percentage'].mean(),\n",
    "                'avg_multiple_exact_percentage': exam_df['multiple_exact_correct_percentage'].mean(),\n",
    "            }\n",
    "            \n",
    "            # Add skipped questions statistics for condition 2\n",
    "            if condition == 'condition_2' and 'skipped_questions_count' in exam_df.columns:\n",
    "                stats_dict.update({\n",
    "                    'avg_skipped': exam_df['skipped_questions_count'].mean(),\n",
    "                    'total_skipped': exam_df['skipped_questions_count'].sum()\n",
    "                })\n",
    "            \n",
    "            stats_data.append(stats_dict)\n",
    "\n",
    "    # 2. Stats by model grouping (Chinese vs Western)\n",
    "    chinese_models = ['qwen', 'ernie', 'deepseek', 'kimi']\n",
    "    western_models = ['gpt', 'anthropic', 'mistral', 'gemini']\n",
    "    \n",
    "    for condition, df in condition_dfs.items():\n",
    "        df_scores = calculate_scores(condition, condition_dfs)\n",
    "        \n",
    "        for region, models in [('Chinese', chinese_models), ('Western', western_models)]:\n",
    "            for exam_type in ['policy', 'comprehensive']:\n",
    "                region_scores = df_scores[\n",
    "                    (df_scores['Model'].isin(models)) & \n",
    "                    (df_scores['exam_type'] == exam_type)\n",
    "                ]\n",
    "                stats_dict = {\n",
    "                    'analysis_type': 'region',\n",
    "                    'condition': condition,\n",
    "                    'exam_type': exam_type,\n",
    "                    'region': region,\n",
    "                    'avg_total': region_scores['total_score(out of 100)'].mean(),\n",
    "                    'median_total': region_scores['total_score(out of 100)'].median(),\n",
    "                    'std_total': region_scores['total_score(out of 100)'].std(),\n",
    "                    'avg_single': region_scores['single_score(out of 60)'].mean(),\n",
    "                    'median_single': region_scores['single_score(out of 60)'].median(),\n",
    "                    'avg_multiple': region_scores['multiple_score(out of 40)'].mean(),\n",
    "                    'median_multiple': region_scores['multiple_score(out of 40)'].median(),\n",
    "                    'avg_single_percentage': region_scores['single_correct_percentage'].mean(),\n",
    "                    'avg_multiple_exact_percentage': region_scores['multiple_exact_correct_percentage'].mean(),\n",
    "                }\n",
    "                \n",
    "                # Add skipped questions statistics for condition 2\n",
    "                if condition == 'condition_2' and 'skipped_questions_count' in region_scores.columns:\n",
    "                    stats_dict.update({\n",
    "                        'avg_skipped': region_scores['skipped_questions_count'].mean(),\n",
    "                        'total_skipped': region_scores['skipped_questions_count'].sum()\n",
    "                    })\n",
    "                \n",
    "                stats_data.append(stats_dict)\n",
    "\n",
    "    \n",
    "    # Convert to DataFrames and round numeric columns\n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    \n",
    "    # Round numeric columns\n",
    "    numeric_cols = stats_df.select_dtypes(include=['float64']).columns\n",
    "    stats_df[numeric_cols] = stats_df[numeric_cols].round(3)\n",
    "    \n",
    "    return stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_scores(df):\n",
    "    # Create a mapping for region groups\n",
    "    region_mapping = {\n",
    "        'gemini': 'Western',\n",
    "        'gpt': 'Western',\n",
    "        'mistral': 'Western',\n",
    "        'anthropic': 'Western',\n",
    "        'ernie': 'Chinese',\n",
    "        'qwen': 'Chinese',\n",
    "        'kimi': 'Chinese',\n",
    "        'deepseek': 'Chinese'\n",
    "    }\n",
    "    \n",
    "    # Select relevant columns and rename them for melting\n",
    "    df_subset = df[['exam_type', 'Model', \n",
    "                    'total_score(out of 100)',\n",
    "                    'single_correct_percentage', \n",
    "                    'multiple_exact_correct_percentage',\n",
    "                    'single_score(out of 60)', \n",
    "                    'multiple_score(out of 40)']]\n",
    "    \n",
    "    # Melt all metrics into a single column\n",
    "    df_long = pd.melt(\n",
    "        df_subset,\n",
    "        id_vars=['exam_type', 'Model', 'total_score(out of 100)'],\n",
    "        value_vars=[\n",
    "            'single_correct_percentage',\n",
    "            'multiple_exact_correct_percentage',\n",
    "            'single_score(out of 60)',\n",
    "            'multiple_score(out of 40)'\n",
    "        ],\n",
    "        var_name='metric_type',\n",
    "        value_name='score/percentage'\n",
    "    )\n",
    "    \n",
    "    # Add region group\n",
    "    df_long['region_group'] = df_long['Model'].map(region_mapping)\n",
    "    \n",
    "    # Clean up metric_type names\n",
    "    metric_mapping = {\n",
    "        'single_correct_percentage': 'single_percentage',\n",
    "        'multiple_exact_correct_percentage': 'multiple_percentage',\n",
    "        'single_score(out of 60)': 'single_score',\n",
    "        'multiple_score(out of 40)': 'multiple_score'\n",
    "    }\n",
    "    df_long['metric_type'] = df_long['metric_type'].map(metric_mapping)\n",
    "    \n",
    "    # Reorder columns\n",
    "    df_long = df_long[['exam_type', 'Model', 'region_group', 'metric_type', 'score/percentage', 'total_score(out of 100)']]\n",
    "    \n",
    "    return df_long\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_scores(scores_dict):\n",
    "    \"\"\"\n",
    "    Combine condition_1 and condition_2 into one DataFrame\n",
    "    \"\"\"\n",
    "    # Add condition column to each DataFrame\n",
    "    dfs = []\n",
    "    for condition in ['condition_1', 'condition_2']:\n",
    "        if condition in scores_dict:\n",
    "            df_copy = scores_dict[condition].copy()\n",
    "            df_copy['condition'] = condition\n",
    "            dfs.append(df_copy)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    combined_c1c2 = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return combined_c1c2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_condition_2_table(df):\n",
    "    # Filter for Condition 2 data\n",
    "    condition_2_df = df[df['condition'] == 'condition_2'].copy()\n",
    "    \n",
    "    # Calculate total questions skipped\n",
    "    condition_2_df['Total questions skipped'] = condition_2_df['skipped_questions_count']\n",
    "    \n",
    "    # Calculate total percent correct with skip\n",
    "    condition_2_df['Total percent correct-w/Skip'] = condition_2_df['total_correct_count'] / (80- condition_2_df['skipped_questions_count']) \n",
    "    \n",
    "    # Calculate total percent correct without skip\n",
    "    # This assumes skipped questions are counted as incorrect\n",
    "    condition_2_df['Total percent correct w/o Skip'] = condition_2_df['total_correct_count'] / 80\n",
    "    \n",
    "    # Calculate ranks for condition 1 and 2 using min method\n",
    "    condition_1_df = df[df['condition'] == \"condition_1\"].copy()\n",
    "    condition_1_df['rank'] = condition_1_df['total_score(out of 100)'].rank(method='min', ascending=False)\n",
    "    condition_2_df['rank'] = condition_2_df['Total percent correct w/o Skip'].rank(method='min', ascending=False)\n",
    "    \n",
    "    # Create rank change mapping\n",
    "    rank_changes = {}\n",
    "    for model in df['Model'].unique():\n",
    "        rank_1 = condition_1_df[condition_1_df['Model'] == model]['rank'].iloc[0]\n",
    "        rank_2 = condition_2_df[condition_2_df['Model'] == model]['rank'].iloc[0]\n",
    "        rank_changes[model] = rank_1 - rank_2\n",
    "    \n",
    "    condition_2_df['Change in Rank'] = condition_2_df['Model'].map(rank_changes)\n",
    "    \n",
    "    # Select and order columns for final display\n",
    "    result = condition_2_df[['Model', \n",
    "                            'Total questions skipped',\n",
    "                            'Total percent correct-w/Skip',\n",
    "                            'Total percent correct w/o Skip',\n",
    "                            'Change in Rank']]\n",
    "    \n",
    "    # Sort by Total percent correct-w/Skip\n",
    "    result = result.sort_values('Total percent correct w/o Skip', ascending=False)\n",
    "    \n",
    "    # Add rank column using dense ranking\n",
    "    result.insert(0, 'Condition 2 Rank', condition_2_df['rank'])\n",
    "    \n",
    "    # Round numeric columns\n",
    "    result = result.round(3)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Export Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_combined_csv(scores_dict):\n",
    "    \"\"\"\n",
    "    Combine all conditions into one DataFrame and save to a single CSV\n",
    "    \"\"\"\n",
    "    # Add condition column to each DataFrame\n",
    "    dfs = []\n",
    "    for condition, df in scores_dict.items():\n",
    "        df_copy = df.copy()\n",
    "        df_copy['condition'] = condition\n",
    "        dfs.append(df_copy)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    combined_df.to_csv('Frontier_all_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load and process data\n",
    "    condition_dfs = load_and_process_files()\n",
    "    \n",
    "    # Calculate scores for each condition\n",
    "    scores = {\n",
    "        'condition_1': calculate_scores('condition_1', condition_dfs),\n",
    "        'condition_2': calculate_scores('condition_2', condition_dfs),\n",
    "        'condition_3': calculate_scores('condition_3', condition_dfs, include_confidence=False)\n",
    "    }\n",
    "    \n",
    "    # Generate statistics and analysis\n",
    "    stats_df = calculate_detailed_statistics(condition_dfs)\n",
    "    \n",
    "    # Save results\n",
    "    save_combined_csv(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base_research_env)",
   "language": "python",
   "name": "base_research_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
