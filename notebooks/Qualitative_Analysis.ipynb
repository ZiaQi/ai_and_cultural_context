{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Analysis of LLM Performance\n",
    "\n",
    "This notebook analyzes how different LLMs perform on the Chinese Social Work Licensing Exam questions.\n",
    "1. Load and clean the raw results data\n",
    "2. Generate balanced samples for qualitative analysis \n",
    "3. Process the labeled results\n",
    "4. Analyze distributions across exam types and question types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_answer(answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean Model_Answer by removing spaces and commas\n",
    "    Example: 'A, B, C' -> 'ABC'\n",
    "    \"\"\"\n",
    "    if pd.isna(answer):  # Handle NaN values\n",
    "        return answer\n",
    "    return ''.join(char for char in str(answer) if char.isalpha())\n",
    "\n",
    "def clean_confidence_value(x):\n",
    "    \"\"\"\n",
    "    Convert confidence values to float, handling various formats\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            # Remove any non-numeric characters except decimal point\n",
    "            x = ''.join(c for c in x if c.isdigit() or c == '.')\n",
    "        return float(x)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def load_and_process_files() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV files for condition1 and process them\n",
    "    Returns a single dataframe with the processed data\n",
    "    \"\"\"\n",
    "    # List to store dataframes\n",
    "    dfs = []\n",
    "    \n",
    "    # Get all CSV files\n",
    "    csv_files = glob.glob(r'/Users/ziaqi/Library/CloudStorage/OneDrive-Personal/Data Projects 2024/chinese_sw_licensing_exam/frontier_results_datafiles/results_*.csv')\n",
    "    \n",
    "    for file in csv_files:\n",
    "        # Get just the filename without the path\n",
    "        filename = os.path.basename(file)\n",
    "        # Parse filename components\n",
    "        parts = filename.replace('.csv', '').split('_')\n",
    "        \n",
    "        if len(parts) != 6:\n",
    "            print(f\"Skipping {file} - parts are not matched in filename\")\n",
    "            continue\n",
    "            \n",
    "        # Extract required information from filename\n",
    "        exam_type = parts[1]      # policy or comprehensive\n",
    "        model = parts[2]          # qwen, mistral, etc.\n",
    "        condition = parts[3]+parts[4]     # condition1, condition2, etc.\n",
    "        question_type = parts[5]  # Add question type from parts[5]\n",
    "        \n",
    "        # Check if this is a condition1 file\n",
    "        try:\n",
    "            condition_index = parts.index('condition')\n",
    "            condition_num = parts[condition_index + 1]\n",
    "            if condition_num != '1':  # Skip if not condition1\n",
    "                continue\n",
    "        except ValueError:\n",
    "            print(f\"Could not find condition number in {filename}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing: {filename}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Clean Model_Answer column\n",
    "        df['Model_Answer'] = df['Model_Answer'].apply(clean_answer)\n",
    "        df['Model_Confidence'] = df['Model_Confidence'].apply(clean_confidence_value)\n",
    "        df['Correct_Answer'] = df['Correct_Answer'].apply(clean_answer)\n",
    "        \n",
    "        # Add new columns\n",
    "        df['Condition'] = condition\n",
    "        df['Exam_Type'] = exam_type\n",
    "        df['Question_Type'] = question_type\n",
    "        \n",
    "        # Add comparison column\n",
    "        df['Is_Correct'] = df['Model_Answer'] == df['Correct_Answer']\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if dfs:\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Concatenated dataframe: {len(final_df)} rows\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"Warning: No condition1 files found\")\n",
    "        return pd.DataFrame()  # Return empty dataframe if no files found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "df = load_and_process_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Samples for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create samples based on specified rules and split into True/False dataframes.\n",
    "    Each sample record will have a unique ID.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with all records\n",
    "        \n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: (true_samples, false_samples)\n",
    "    \"\"\"\n",
    "    # Initialize empty lists to store samples\n",
    "    true_samples = []\n",
    "    false_samples = []\n",
    "    \n",
    "    # Get unique combinations of model and exam type\n",
    "    models = df['Model'].unique()\n",
    "    exam_types = df['Exam_Type'].unique()\n",
    "    \n",
    "    for model in models:\n",
    "        # Get subset for this model\n",
    "        subset = df[df['Model'] == model]\n",
    "        \n",
    "        # Randomly sample correct answers (n=30)\n",
    "        correct_answers = subset[subset['Is_Correct'] == True]\n",
    "        if len(correct_answers) > 0:\n",
    "            true_samples.append(correct_answers.sample(n=min(30, len(correct_answers)), random_state=1))\n",
    "        \n",
    "        # Sample all incorrect answers that do not contain \"跳过\" in \"Model_Answer\"\n",
    "        incorrect_answers = subset[(subset['Is_Correct'] == False) & (~subset['Model_Answer'].str.contains(\"跳过\", na=False))]\n",
    "        if len(incorrect_answers) > 0:\n",
    "            false_samples.append(incorrect_answers)\n",
    "    \n",
    "    # Combine all samples\n",
    "    true_samples_df = pd.concat(true_samples, ignore_index=True) if true_samples else pd.DataFrame()\n",
    "    false_samples_df = pd.concat(false_samples, ignore_index=True) if false_samples else pd.DataFrame()\n",
    "    \n",
    "    # Add new columns \"code\" and \"comment\" with no values\n",
    "    true_samples_df['code'] = None\n",
    "    true_samples_df['comment'] = None\n",
    "    false_samples_df['code'] = None\n",
    "    false_samples_df['comment'] = None\n",
    "\n",
    "    # Sort samples by Exam_Type, Question_Type, Model\n",
    "    true_samples_df = true_samples_df.sort_values(by=['Model','Exam_Type', 'Question_Type'], ignore_index=True, ascending=[True, True, False])\n",
    "    false_samples_df = false_samples_df.sort_values(by=['Model','Exam_Type', 'Question_Type'], ignore_index=True, ascending=[True, True, False])\n",
    "\n",
    "    # Add Sample_ID to combined dataframes\n",
    "    if not true_samples_df.empty:\n",
    "        true_samples_df['Sample_ID'] = [f\"S{i:03d}\" for i in range(1, len(true_samples_df) + 1)]\n",
    "    if not false_samples_df.empty:\n",
    "        false_samples_df['Sample_ID'] = [f\"S{i:03d}\" for i in range(len(true_samples_df) + 1, \n",
    "                                                                   len(true_samples_df) + len(false_samples_df) + 1)]\n",
    "        \n",
    "    \n",
    "    # Print sampling statistics\n",
    "    print(\"\\nSampling Statistics:\")\n",
    "    print(f\"Total True Samples: {len(true_samples_df)}\")\n",
    "    print(f\"Total False Samples: {len(false_samples_df)}\")\n",
    "    \n",
    "    # Print detailed breakdown by model and exam type\n",
    "    print(\"\\nDetailed Breakdown:\")\n",
    "    for model in models:\n",
    "        true_count = len(true_samples_df[true_samples_df['Model'] == model])\n",
    "        false_count = len(false_samples_df[false_samples_df['Model'] == model])\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(f\"True Samples: {true_count}\")\n",
    "        print(f\"False Samples: {false_count}\")\n",
    "    \n",
    "    return true_samples_df, false_samples_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage:\n",
    "true_samples, false_samples = create_samples(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_samples = true_samples[['Sample_ID', 'Question_ID', 'Question', 'Selections', 'Correct_Answer', 'Model_Answer', 'Official_Explanation', 'Model_Explanation', 'Model', 'Condition', 'Exam_Type', 'Question_Type', 'Is_Correct','code','comment']]\n",
    "false_samples = false_samples[['Sample_ID', 'Question_ID', 'Question', 'Selections', 'Correct_Answer', 'Model_Answer', 'Official_Explanation', 'Model_Explanation', 'Model', 'Condition', 'Exam_Type', 'Question_Type', 'Is_Correct','code','comment']]\n",
    "true_samples.to_csv('samples/sample_correct.csv', index=False, encoding='utf-8-sig')\n",
    "false_samples.to_csv('samples/sample_incorrect.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample n=30 Records for Reliability Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 30 records from true_samples and false_samples\n",
    "true_raliability = true_samples.sample(n=30, random_state=1)\n",
    "false_reliability = false_samples.sample(n=30, random_state=1)\n",
    "\n",
    "# Keep only the required columns\n",
    "true_raliability = true_raliability[['Sample_ID','Question', 'Selections', 'Correct_Answer', 'Model_Answer', 'Official_Explanation', 'Model_Explanation', 'code','comment']]\n",
    "false_reliability = false_reliability[['Sample_ID','Question', 'Selections', 'Correct_Answer', 'Model_Answer', 'Official_Explanation', 'Model_Explanation', 'code','comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "true_raliability.to_csv('samples/sample_correct_reliability.csv', index=False, encoding='utf-8-sig')\n",
    "false_reliability.to_csv('samples/sample_incorrect_reliability.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle Remaining Records and Create Sub-files for Raters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract records used for reliability testing from true_samples and false_samples\n",
    "true_samples_rater = true_samples[~true_samples['Sample_ID'].isin(true_raliability['Sample_ID'])]\n",
    "false_samples_rater = false_samples[~false_samples['Sample_ID'].isin(false_reliability['Sample_ID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle records\n",
    "true_samples_rater = true_samples_rater.sample(frac=1, random_state=77).reset_index(drop=True)\n",
    "false_samples_rater = false_samples_rater.sample(frac=1, random_state=77).reset_index(drop=True)\n",
    "\n",
    "# Keep only the required columns\n",
    "true_samples_rater = true_samples_rater[['Sample_ID','Question', 'Selections', 'Correct_Answer', 'Model_Answer', 'Official_Explanation', 'Model_Explanation', 'code','comment']]\n",
    "false_samples_rater = false_samples_rater[['Sample_ID','Question', 'Selections', 'Correct_Answer', 'Model_Answer', 'Official_Explanation', 'Model_Explanation', 'code','comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save true_samples and false_samples to CSV files with each file containing 100 records\n",
    "batch_size = 100\n",
    "num_batches_true = len(true_samples_rater) // batch_size + 1\n",
    "num_batches_false = len(false_samples_rater) // batch_size + 1\n",
    "\n",
    "for i in range(num_batches_true):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    true_samples_rater.iloc[start_idx:end_idx].to_csv(f'samples/subsample_correct_batch{i+1}.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "for i in range(num_batches_false):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    false_samples_rater.iloc[start_idx:end_idx].to_csv(f'samples/subsample_incorrect_batch{i+1}.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data after Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read labeled data files into a dataframe uisng glob\n",
    "labeled_files = glob.glob('/qualitative_analysis/labeled/labeled_*.csv')\n",
    "# Turn the two files into two dataframes\n",
    "labeled_dfs = [pd.read_csv(file) for file in labeled_files]\n",
    "# Access the \"correct\" and \"incorrect\" dataframes\n",
    "labeled_correct = labeled_dfs[0]\n",
    "labeled_incorrect = labeled_dfs[1]\n",
    "\n",
    "# Correct the column names \"QuestionID\" to \"Sample_ID\"\n",
    "labeled_correct = labeled_correct.rename(columns={'QuestionID': 'Sample_ID'})\n",
    "labeled_incorrect = labeled_incorrect.rename(columns={'QuestionID': 'Sample_ID'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge labeling results from raters into a single column\n",
    "labeled_correct['label'] = labeled_correct.apply(lambda row: row['Zia'] if pd.notna(row['Zia']) else row['Miao'], axis=1)\n",
    "\n",
    "labeled_incorrect['label'] = labeled_incorrect.apply(\n",
    "    lambda row: row['Sitao'] if pd.notna(row['Sitao']) else (\n",
    "        row['Cao'] if pd.notna(row['Cao']) else (\n",
    "            row['Miao'] if pd.notna(row['Miao']) else row['Zia']\n",
    "        )\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# check for any missing labels\n",
    "print(labeled_correct[labeled_correct['label'].isna()])\n",
    "print(labeled_incorrect[labeled_incorrect['label'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with true_samples and false_samples to add fields \"Exam_Type\" and \"Question_Type\"\n",
    "labeled_correct = labeled_correct.merge(true_samples[['Sample_ID', 'Exam_Type', 'Question_Type']], on='Sample_ID', how='left')[['Sample_ID', 'Question', 'Options', 'Correct_Answer', 'Official_Explanation', 'Model_Explanation', 'Model', 'label', 'Exam_Type', 'Question_Type']]\n",
    "labeled_incorrect = labeled_incorrect.merge(false_samples[['Sample_ID', 'Exam_Type', 'Question_Type']], on='Sample_ID', how='left')[['Sample_ID', 'Question', 'Options', 'Correct_Answer', 'Model_Answer', 'Official_Explanation', 'Model_Explanation', 'Model', 'label', 'Exam_Type', 'Question_Type']]\n",
    "# check the data shape\n",
    "labeled_correct.shape, labeled_incorrect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dataframe for visualization\n",
    "labeled_correct['Is_Correct'] = True\n",
    "labeled_incorrect['Is_Correct'] = False\n",
    "labeled_combined = pd.concat([labeled_correct, labeled_incorrect], ignore_index=True)[['Sample_ID', 'Model', 'label', 'Exam_Type', 'Question_Type', 'Is_Correct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "labeled_correct.to_csv('labeled/clean_labeled_correct.csv', index=False, encoding='utf-8-sig')\n",
    "labeled_incorrect.to_csv('labeled/clean_labeled_incorrect.csv', index=False, encoding='utf-8-sig')\n",
    "labeled_combined.to_csv('labeled/clean_labeled_combined.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
